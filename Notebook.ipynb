#### Libraries
import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor,ExtraTreesRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer
from sklearn.svm import SVR
from sklearn.multioutput import MultiOutputRegressor

"""## PROBLEM OVERVIEW

"""

# IMPORT THE DATASET DEVELOPMENT
df = pd.read_csv('development.csv')
#print(df)
#df.info()
#df.describe()


# IMPORT THE DATASET EVALUATION
df_eval = pd.read_csv('evaluation.csv')
#print(df_eval)
#df_eval.info()
#df_eval.describe()


## SPATIAL VISUALIZATION OF THE DATASET
## group by of x and y to visualize spatially all the existent positions in the area of the sensor
#df_grouped = df.groupby(['x', 'y']).mean().reset_index()
# plt.figure(figsize=(13, 10))
# plot = plt.scatter(df_grouped['x'], df_grouped['y'])
# # Title and  axis labes
# plt.title('Spatial graph of coordinates x and y')
# plt.xlabel('Coordinate x')
# plt.ylabel('Coordinate y')


## DETECTION OF ANOMALIES IN PADS
# # Correlation matrix
# for i in range(18):
#     #selecting the features of each pad
#     features = [f'pmax[{i}]',f'negpmax[{i}]',f'area[{i}]',f'tmax[{i}]',f'rms[{i}]']
#     correlation_matrix = df[features].corr()
#
#     plt.figure(figsize=(7, 6))
#     sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
#     plt.title(f'Correlation matrix of pad {i}')
#     plt.show()

# # Boxplot of tmax and rms
# for i in range(18):
#     plt.figure(figsize=(7, 6))
#     sns.boxplot(data=df[f'tmax[{i}]']) # or sns.boxplot(data=df[f'rms[{i}]'])
#     plt.title(f'Boxplot of tmax[{i}]')
#     plt.show()


# # CHECK THE BALANCE OF THE DATASET
# count_per_position = df.groupby(['x', 'y']).size().reset_index(name='count')
# #print(count_per_position)





"""## PREPROCESSING"""

# REMOVING THE FEATURES FROM THE NOISY PADS
columns_to_remove = [f'tmax[{i}]' for i in [0, 7, 12, 15, 16, 17]] + \
                    [f'pmax[{i}]' for i in [0, 7, 12, 15, 16, 17]] + \
                    [f'negpmax[{i}]' for i in [0, 7, 12, 15, 16, 17]] + \
                    [f'rms[{i}]' for i in [0, 7, 12, 15, 16, 17]] + \
                    [f'area[{i}]' for i in [0, 7, 12, 15, 16, 17]]
# Removing them from both datasets
df_correct = df.drop(columns=columns_to_remove)
df_eval = df_eval.drop(columns=columns_to_remove)


# CHECK FOR INCORRECT DATA
indices = [1,2,3,4,5,6,8,9,10,11,13,14]
for i in indices:
    df_correct = df_correct[df_correct[f'negpmax[{i}]'] <= 0]  # removing rows with negpmax > 0
    df_correct = df_correct[df_correct[f'pmax[{i}]'] >= 0]  # removing rows with pmax < 0


## CHECK FOR OUTLIERS
# # group by of x and y to visualize spatially all the existent positions in the area of the sensor
# df_grouped = df_correct.groupby(['x', 'y']).mean().reset_index()
# feature = 'negpmax[1]'  # name of the feature you want to check
# plt.figure(figsize=(13, 10))
# # plot based on a feature of the dataset
# plot = plt.scatter(df_grouped['x'], df_grouped['y'],c=df_grouped[feature], cmap='viridis')
# # Color bar
# plt.colorbar(plot, label=feature)
# # Title and  axis labes
# plt.title(f'Spatial graph of coordinates x and y according to {feature} ')
# plt.xlabel('Coordinate x')
# plt.ylabel('Coordinate y')


# ELIMINATION OF OUTLIERS
# Creation of a mask to apply to the dataset
outliers_elimination = (
    (df_correct['negpmax[1]'] < -70) |
    (df_correct['negpmax[2]'] < -40) |
    (df_correct['negpmax[3]'] < -80) |
    (df_correct['negpmax[4]'] < -50) |
    (df_correct['negpmax[5]'] < -80) |
    (df_correct['negpmax[6]'] < -75) |
    (df_correct['negpmax[8]'] < -75) |
    (df_correct['negpmax[9]'] < -70) |
    (df_correct['negpmax[10]'] < -85) |
    (df_correct['negpmax[11]'] < -80) |
    (df_correct['negpmax[13]'] < -80) |
    (df_correct['negpmax[14]'] < -70) )

df_correct = df_correct[~outliers_elimination]
#print(df_correct)


# ADD NEW FEATURES
indices = [1,2,3,4,5,6,8,9,10,11,13,14]
# Add the columns diff[i] at both datasets
for i in indices:
    df_correct[f'range[{i}]'] = (df_correct[f'pmax[{i}]'] - df_correct[f'negpmax[{i}]'])
    df_eval[f'range[{i}]'] = (df_eval[f'pmax[{i}]'] - df_eval[f'negpmax[{i}]'])


# # FEATURE IMPORTANCE
# X = df_correct.drop(['x', 'y'], axis=1)
# y = df_correct[['x', 'y']]
# # Creation of the model
# model = RandomForestRegressor() # or model = ExtraTreesRegressor()
# # Training
# model.fit(X, y)
# # Feature importance
# feature_importance = model.feature_importances_
# # Creating a dataset for the feature importance
# feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importance})
# # Ordering by importance
# feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)


# ELIMINATION OF FEATURES ACCORDING TO FEATURE IMPORTANCE
columns_to_remove = [f'tmax[{i}]' for i in [1,2,3,4,5,6,8,9,10,11,13,14]] + \
                    [f'rms[{i}]' for i in [1,2,3,4,5,6,8,9,10,11,13,14]]
#removing them from both datasets
df_correct = df_correct.drop(columns=columns_to_remove)
df_eval = df_eval.drop(columns=columns_to_remove)






# """## HYPER-PARAMETERS TUNING
#
#
# """
#
# #CREATION OF THE SUBSET OF THE DATASET
# #taking the first 40 values of each position x y
# subset  = df_correct.groupby(['x', 'y'], group_keys=False).apply(lambda group: group.head(40)).reset_index(drop=True)
#
# #CREATION OF TRAINING/TEST SET
# #selection of all the combination of x and y
# combinations = subset[['x', 'y']].drop_duplicates()
# # Split the combinations in train and test
# train_combinations, test_combinations = train_test_split(combinations, test_size=0.2, random_state=42)
# #Df containing only rows of 'subset' corresponding to combinations of 'x' and 'y' in train_combinations
# train_set = pd.merge(subset, train_combinations, on=['x', 'y'])
# #Df containing only rows of 'subset' corresponding to combinations of 'x' and 'y' in test_combinations
# test_set = pd.merge(subset, test_combinations, on=['x', 'y'])
# X_train = train_set.drop(['x', 'y'], axis = 1)
# y_train = train_set[['x', 'y']]
# X_test = test_set.drop(['x', 'y'], axis = 1)
# y_test = test_set[['x', 'y']]
#
#
# #DISTANCE USED AS CRITERION FOR THE GRID SEARCH
# def custom_distance(y_true, y_pred):
#     distances = np.sqrt(np.sum((np.array(y_true) - np.array(y_pred))**2, axis=1))
#     return np.mean(distances)
#
#
# # GRIDSEARCHCV [SVR]
# # Parameters
# param_grid = {
#     'estimator__kernel': ['poly', 'rbf', 'sigmoid'],
#     'estimator__gamma': ['scale'],
#     'estimator__C': [1, 10, 50, 100, 500],
#     }
# reg = MultiOutputRegressor(SVR(), n_jobs=-1)
# #defining the scorer
# score = make_scorer(custom_distance, greater_is_better=False)
# #Initialise GridSearchCV for searching hyperparameters
# gs = GridSearchCV(estimator=reg, param_grid=param_grid, scoring=score, cv=5, verbose=4, n_jobs=-1)
# # Perform hyper-parameter search on the training set
# gs.fit(X_train, y_train)
# # Print best found parameters
# print("Best hyperparameters:", gs.best_params_)
#
# best_svr_reg = SVR(**gs.best_params_)
# best_svr_reg.fit(X_train, y_train)
# y_pred = best_svr_reg.predict(X_test)
# print(f"Best score on test set: {custom_distance(y_test, y_pred)}")
#
#
# # GRIDSEARCHCV  [Random Forest Regressor]
# # Parameters
# param_grid = {
#     'estimator__n_estimators': [100, 300, 500],
#     'estimator__max_features': ['sqrt', 'log2'],
#     'estimator__max_depth': [15, 30, 40, None],
#     'estimator__min_samples_split': [2, 5, 10],
#     'estimator__min_samples_leaf': [1, 2, 4],
#     'estimator__random_state': [42],
#     'estimator__bootstrap': [True, False],
#     'estimator__n_jobs': [-1]
#     }
# #defining the scorer
# score = make_scorer(custom_distance, greater_is_better=False)
# #Initialise GridSearchCV for searching hyperparameters
# gs = GridSearchCV(RandomForestRegressor(), param_grid, scoring=score, cv=5, verbose=4, n_jobs=-1)
# # Perform hyper-parameter search on the training set
# gs.fit(X, y)
# # Print best found parameters
# print("Best hyperparameters:", gs.best_params_)
#
# best_rf_reg = RandomForestRegressor(**gs.best_params_)
# best_rf_reg.fit(X_train, y_train)
# y_pred = best_rf_reg.predict(X_test)
# print(f"Best score on test set: {custom_distance(y_test, y_pred)}")
#
#
# # GRIDSEARCHCV  [Extra Trees Regressor]
# # Parameters
# param_grid = {
#     'estimator__n_estimators': [100, 300, 500],
#     'estimator__max_features': ['sqrt', 'log2'],
#     'estimator__max_depth': [15, 30, 40, None],
#     'estimator__min_samples_split': [2, 5, 10],
#     'estimator__min_samples_leaf': [1, 2, 4],
#     'estimator__random_state': [42],
#     'estimator__bootstrap': [True, False],
#     'estimator__n_jobs': [-1]
#     }
# #defining the scorer
# score = make_scorer(custom_distance, greater_is_better=False)
# #Initialise GridSearchCV for searching hyperparameters
# gs = GridSearchCV(ExtraTreesRegressor(), param_grid, scoring=score, cv=5, verbose=4, n_jobs=-1)
# # Perform hyper-parameter search on the training set
# gs.fit(X, y)
# # Print best found parameters
# print("Best hyperparameters:", gs.best_params_)
#
# best_etr_reg = ExtraTreesRegressor(**gs.best_params_)
# best_etr_reg.fit(X_train, y_train)
# y_pred = best_etr_reg.predict(X_test)
# print(f"Best score on test set: {custom_distance(y_test, y_pred)}")




"""## PREDICTION ON THE EVALUATION DATASET"""

#DIVISION OF X_train AND Y_train
X_train = df_correct.drop(['x', 'y'], axis = 1)
y_train = df_correct[['x', 'y']]


#PREPARATION OF TEST SET
X_test = df_eval.drop(['Id'], axis = 1)
Indexes = df_eval['Id']


#DEFINITION OF MODELS WITH BEST PARAMETERS
base_estimator = ExtraTreesRegressor(n_estimators=500,
                                     max_depth=40,
                                     min_samples_split=2,
                                     min_samples_leaf=1,
                                     max_features='sqrt',
                                     random_state=42,
                                     bootstrap=False,
                                     n_jobs=-1
                                     )

# base_estimator = RandomForestRegressor(n_estimators = 500,
#                                        max_depth=30,
#                                        max_features='sqrt',
#                                        min_samples_split=2,
#                                        min_samples_leaf=1,
#                                        random_state = 42,
#                                        bootstrap= False
#                                        n_jobs=-1,
#                                       )

reg = MultiOutputRegressor(base_estimator)  # MultiOutputRegressor

#TRAINING

reg.fit(X_train, y_train)

#PREDICTIONS

y_pred = reg.predict(X_test)

#FORMAT OUTPUT

formatted_predictions = []
for coord in y_pred:
    formatted_predictions.append(f"{coord[0]}|{coord[1]}")

#FINAL CSV FILE TO UPLOAD

pd.DataFrame(formatted_predictions, index=Indexes).to_csv("output.csv", index_label="Id", header=["Predicted"])
